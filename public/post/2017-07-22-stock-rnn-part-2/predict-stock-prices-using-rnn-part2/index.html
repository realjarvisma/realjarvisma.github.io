<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Predict Stock Prices Using Rnn Part 2 | Jarvis' Log</title>
<meta name=keywords content="tutorial,rnn,tensorflow"><meta name=description content="In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in Part 1 with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.

Dataset
During the search, I found this library for querying Yahoo! Finance API. It would be very useful if Yahoo hasn’t shut down the historical data fetch API. You may find it useful for querying other information though. Here I pick the Google Finance link, among a couple of free data sources for downloading historical stock prices."><meta name=author content="Jarvis Ma"><link rel=canonical href=http://localhost:1313/post/2017-07-22-stock-rnn-part-2/predict-stock-prices-using-rnn-part2/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/post/2017-07-22-stock-rnn-part-2/predict-stock-prices-using-rnn-part2/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><meta property="og:title" content="Predict Stock Prices Using Rnn Part 2"><meta property="og:description" content="In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in Part 1 with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.

Dataset
During the search, I found this library for querying Yahoo! Finance API. It would be very useful if Yahoo hasn’t shut down the historical data fetch API. You may find it useful for querying other information though. Here I pick the Google Finance link, among a couple of free data sources for downloading historical stock prices."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/2017-07-22-stock-rnn-part-2/predict-stock-prices-using-rnn-part2/"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-07-22T02:19:57+08:00"><meta property="article:modified_time" content="2017-07-22T02:19:57+08:00"><meta property="og:site_name" content="Jarvis' Log"><meta name=twitter:card content="summary"><meta name=twitter:title content="Predict Stock Prices Using Rnn Part 2"><meta name=twitter:description content="In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in Part 1 with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.

Dataset
During the search, I found this library for querying Yahoo! Finance API. It would be very useful if Yahoo hasn’t shut down the historical data fetch API. You may find it useful for querying other information though. Here I pick the Google Finance link, among a couple of free data sources for downloading historical stock prices."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/post/"},{"@type":"ListItem","position":2,"name":"Predict Stock Prices Using Rnn Part 2","item":"http://localhost:1313/post/2017-07-22-stock-rnn-part-2/predict-stock-prices-using-rnn-part2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Predict Stock Prices Using Rnn Part 2","name":"Predict Stock Prices Using Rnn Part 2","description":"In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in Part 1 with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.\nDataset During the search, I found this library for querying Yahoo! Finance API. It would be very useful if Yahoo hasn’t shut down the historical data fetch API. You may find it useful for querying other information though. Here I pick the Google Finance link, among a couple of free data sources for downloading historical stock prices.\n","keywords":["tutorial","rnn","tensorflow"],"articleBody":"In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in Part 1 with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.\nDataset During the search, I found this library for querying Yahoo! Finance API. It would be very useful if Yahoo hasn’t shut down the historical data fetch API. You may find it useful for querying other information though. Here I pick the Google Finance link, among a couple of free data sources for downloading historical stock prices.\nThe data fetch code can be written as simple as:\n1 2 3 4 5 6 7 8 import urllib2 from datetime import datetime BASE_URL = \"https://www.google.com/finance/historical?\" \"output=csv\u0026q={0}\u0026startdate=Jan+1%2C+1980\u0026enddate={1}\" symbol_url = BASE_URL.format( urllib2.quote('GOOG'), # Replace with any stock you are interested. urllib2.quote(datetime.now().strftime(\"%b+%d,+%Y\"), '+') ) When fetching the content, remember to add try-catch wrapper in case the link fails or the provided stock symbol is not valid.\n1 2 3 4 5 6 try: f = urllib2.urlopen(symbol_url) with open(\"GOOG.csv\", 'w') as fin: print \u003e\u003e fin, f.read() except urllib2.HTTPError: print \"Fetching Failed: {}\".format(symbol_url) Model Construction The model is expected to learn the price sequences of different stocks in time. Due to the different underlying patterns, I would like to tell the model which stock it is dealing with explicitly. Embedding is more favored than one-hot encoding, because:\nGiven that the train set includes $N$ stocks, the one-hot encoding would introduce $N$ (or $N-1$) additional sparse feature dimensions. Once each stock symbol is mapped onto a much smaller embedding vector of length $k$, $k \\ll N$, we end up with a much more compressed representation and smaller dataset to take care of. Since embedding vectors are variables to learn. Similar stocks could be associated with similar embeddings and help the prediction of each others, such as “GOOG” and “GOOGL” which you will see in Fig. 5. later. In the recurrent neural network, at one time step $t$, the input vector contains input_size (labelled as $w$) daily price values of $i$-in stock, ($p_{i, t w}, p_{i, t w+1}, \\ldots, p_{i,(t+1) w-1}$). The stock symbol is uniquely mapped to a vector of length embeddingsize (labelled as $k$), $\\left(e{i, 0}, e*{i, 1}, \\ldots, e*{i, k}\\right)$. As illustrated in Fig. 1., the price vector is concatenated with the embedding vector and then fed into the LSTM cell.\nAnother alternative is to concatenate the embedding vectors with the last state of the LSTM cell and learn new weights $W$ and bias $b$ in the output layer. However, in this way, the LSTM cell cannot tell apart prices of one stock from another and its power would be largely restrained. Thus I decided to go with the former approach.\nFig. 1: The architecture of the stock price prediction RNN model with stock symbol embeddings.\nTwo new configuration settings are added into RNNConfig:\nembedding_size controls the size of each embedding vector; stock_count refers to the number of unique stocks in the dataset. Together they define the size of the embedding matrix, for which the model has to learn embedding_size $\\times $ stock_count additional variables compared to the model in Part 1.\n1 2 3 4 class RNNConfig(): # ... old ones embedding_size = 3 stock_count = 50 Define the Graph — Let’s start going through some code —\n(1) As demonstrated in tutorial Part 1: Define the Graph, let us define a tf.Graph() named lstm_graph and a set of tensors to hold input data, inputs, targets, and learning_rate in the same way. One more placeholder to define is a list of stock symbols associated with the input prices. Stock symbols have been mapped to unique integers beforehand with label encoding.\n1 2 # Mapped to an integer. one label refers to one stock symbol. stock_labels = tf.placeholder(tf.int32, [None, 1]) (2) Then we need to set up an embedding matrix to play as a lookup table, containing the embedding vectors of all the stocks. The matrix is initialized with random numbers in the interval [-1, 1] and gets updated during training.\n1 (2) Then we need to set up an embedding matrix to play as a lookup table, containing the embedding vectors of all the stocks. The matrix is initialized with random numbers in the interval [-1, 1] and gets updated during training. (3) Repeat the stock labels num_steps times to match the unfolded version of RNN and the shape of inputs tensor during training. The transformation operation tf.tile receives a base tensor and creates a new tensor by replicating its certain dimensions multiples times; precisely the $i$-th dimension of the input tensor gets multiplied by multiples[i] times. For example, if the stock_labels is [[0], [0], [2], [1]] tiling it by [1, 5] produces [[0 0 0 0 0], [0 0 0 0 0], [2 2 2 2 2], [1 1 1 1 1]].\n1 stacked_stock_labels = tf.tile(stock_labels, multiples=[1, config.num_steps]) (4) Then we map the symbols to embedding vectors according to the lookup table embedding_matrix.\n1 2 # stock_label_embeds.get_shape() = (?, num_steps, embedding_size). stock_label_embeds = tf.nn.embedding_lookup(embedding_matrix, stacked_stock_labels) (5) Finally, combine the price values with the embedding vectors. The operation tf.concat concatenates a list of tensors along the dimension axis. In our case, we want to keep the batch size and the number of steps unchanged, but only extend the input vector of length input_size to include embedding features.\n1 2 3 4 # inputs.get_shape() = (?, num_steps, input_size) # stock_label_embeds.get_shape() = (?, num_steps, embedding_size) # inputs_with_embeds.get_shape() = (?, num_steps, input_size + embedding_size) inputs_with_embeds = tf.concat([inputs, stock_label_embeds], axis=2) The rest of code runs the dynamic RNN, extracts the last state of the LSTM cell, and handles weights and bias in the output layer. See Part 1: Define the Graph for the details.\nTraining Session Please read Part 1: Start Training Session if you haven’t for how to run a training session in Tensorflow.\nBefore feeding the data into the graph, the stock symbols should be transformed to unique integers with label encoding.\n1 2 3 from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(list_of_symbols) The train/test split ratio remains same, 90% for training and 10% for testing, for every individual stock.\nVisualize the Graph After the graph is defined in code, let us check the visualization in Tensorboard to make sure that components are constructed correctly. Essentially it looks very much like our architecture illustration in Fig. 1.\nFig. 2: TensorBoard visualization of the graph defined above. Two modules, “train” and “save,” have been removed from the main graph.\nOther than presenting the graph structure or tracking the variables in time, Tensorboard also supports embeddings visualization. In order to communicate the embedding values to Tensorboard, we need to add proper tracking in the training logs.\n(0) In my embedding visualization, I want to color each stock with its industry sector. This metadata should stored in a csv file. The file has two columns, the stock symbol and the industry sector. It does not matter whether the csv file has header, but the order of the listed stocks must be consistent with label_encoder.classes_.\n1 2 3 4 5 6 import csv embedding_metadata_path = os.path.join(your_log_file_folder, 'metadata.csv') with open(embedding_metadata_path, 'w') as fout: csv_writer = csv.writer(fout) # write the content into the csv file. # for example, csv_writer.writerows([\"GOOG\", \"information_technology\"]) (1) Set up the summary writer first within the training tf.Session.\n1 2 3 4 from tensorflow.contrib.tensorboard.plugins import projector with tf.Session(graph=lstm_graph) as sess: summary_writer = tf.summary.FileWriter(your_log_file_folder) summary_writer.add_graph(sess.graph) (2) Add the tensor embedding_matrix defined in our graph lstm_graph into the projector config variable and attach the metadata csv file.\n1 2 3 4 5 6 projector_config = projector.ProjectorConfig() # You can add multiple embeddings. Here we add only one. added_embedding = projector_config.embeddings.add() added_embedding.tensor_name = embedding_matrix.name # Link this tensor to its metadata file. added_embedding.metadata_path = embedding_metadata_path (3) This line creates a file projector_config.pbtxt in the folder your_log_file_folder. TensorBoard will read this file during startup.\n1 projector.visualize_embeddings(summary_writer, projector_config) Results The model is trained with top 50 stocks with largest market values in the S\u0026P 500 index.\n1 python main.py --stock_count=50 --embed_size=3 --input_size=3 --max_epoch=50 --train And the following configuration is used:\n1 2 3 4 5 6 7 8 9 10 11 12 stock_count = 100 input_size = 3 embed_size = 3 num_steps = 30 lstm_size = 256 num_layers = 1 max_epoch = 50 keep_prob = 0.8 batch_size = 64 init_learning_rate = 0.05 learning_rate_decay = 0.99 init_epoch = 5 Price Prediction As a brief overview of the prediction quality, Fig. 3 plots the predictions for test data of “KO”, “AAPL”, “GOOG” and “NFLX”. The overall trends matched up between the true values and the predictions. Considering how the prediction task is designed, the model relies on all the historical data points to predict only next 5 (input_size) days. With a small input_size, the model does not need to worry about the long-term growth curve. Once we increase input_size, the prediction would be much harder.\nFig. 3: True and predicted stock prices of AAPL, MSFT, and GOOG in the test set. The prices are normalized across consecutive prediction sliding windows (See Part 1: Normalization.) The y-axis values get multiplied by 5 for a better comparison between true and predicted trends.\nEmbedding Visualization One common technique to visualize the clusters in embedding space is t-SNE (Maaten and Hinton, 2008), which is well supported in Tensorboard. t-SNE, short for “t-Distributed Stochastic Neighbor Embedding, is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002), but with a modified cost function that is easier to optimize.\nSimilar to SNE, t-SNE first converts the high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities. t-SNE defines a similar probability distribution over the data points in the low-dimensional space, and it minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points on the map. Check this post for how to adjust the parameters, Perplexity and learning rate (epsilon), in t-SNE visualization.\nFig. 4: Visualization of the stock embeddings using t-SNE. Each label is colored based on the stock industry sector. We have 5 clusters. Interestingly, GOOG, GOOGL, and FB belong to the same cluster, while AMZN and AAPL stay in another.\nIn the embedding space, we can measure the similarity between two stocks by examining the similarity between their embedding vectors. For example, GOOG is mostly similar to GOOGL in the learned embeddings (See Fig. 5).\nFig. 5: “GOOG” is clicked in the embedding visualization graph, and the top 20 similar neighbors are highlighted with colors from dark to light as the similarity decreases.\nKnown Problems The prediction values get diminished and flatten quite a lot as the training goes. That’s why I multiplied the absolute values by a constant to make the trend is more visible in Fig. 3., as I’m more curious about whether the prediction on the up-or-down direction right. However, there must be a reason for the diminishing prediction value problem. Potentially rather than using simple MSE as the loss, we can adopt another form of loss function to penalize more when the direction is predicted wrong. The loss function decreases fast at the beginning, but it suffers from occasional value explosion (a sudden peak happens and then goes back immediately). I suspect it is related to the form of loss function too. A updated and smarter loss function might be able to resolve the issue. ","wordCount":"1908","inLanguage":"en","datePublished":"2017-07-22T02:19:57+08:00","dateModified":"2017-07-22T02:19:57+08:00","author":{"@type":"Person","name":"Jarvis Ma"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/post/2017-07-22-stock-rnn-part-2/predict-stock-prices-using-rnn-part2/"},"publisher":{"@type":"Organization","name":"Jarvis' Log","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Jarvis' Log (Alt + H)"><img src=http://localhost:1313/favicon.ico alt aria-label=logo height=15>Jarvis' Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Predict Stock Prices Using Rnn Part 2
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2017-07-22 02:19:57 +0800 CST'>Jul 22, 2017</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1908 words&nbsp;·&nbsp;Jarvis Ma&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/post/2017-07-22-stock-rnn-part-2/predict-stock-prices-using-rnn-part2.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#dataset>Dataset</a></li><li><a href=#model-construction>Model Construction</a><ul><li><a href=#define-the-graph>Define the Graph</a></li><li><a href=#training-session>Training Session</a></li><li><a href=#visualize-the-graph>Visualize the Graph</a></li></ul></li><li><a href=#results>Results</a><ul><li><a href=#price-prediction>Price Prediction</a></li><li><a href=#embedding-visualization>Embedding Visualization</a></li><li><a href=#known-problems>Known Problems</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>In the Part 2 tutorial, I would like to continue the topic on stock price prediction and to endow the recurrent neural network that I have built in <a href=https://jarvisma.xyz/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/>Part 1</a> with the capability of responding to multiple stocks. In order to distinguish the patterns associated with different price sequences, I use the stock symbol embedding vectors as part of the input.</p><hr><h2 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h2><p>During the search, I found <a href=https://github.com/lukaszbanasiak/yahoo-finance>this library</a> for querying Yahoo! Finance API. It would be very useful if Yahoo hasn’t shut down the historical data fetch API. You may find it useful for querying other information though. Here I pick the Google Finance link, among <a href=https://www.quantshare.com/sa-43-10-ways-to-download-historical-stock-quotes-data-for-free>a couple of free data sources</a> for downloading historical stock prices.</p><p>The data fetch code can be written as simple as:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3>3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4>4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5>5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6>6</a>
</span><span class=lnt id=hl-0-7><a class=lnlinks href=#hl-0-7>7</a>
</span><span class=lnt id=hl-0-8><a class=lnlinks href=#hl-0-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>urllib2</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl><span class=n>BASE_URL</span> <span class=o>=</span> <span class=s2>&#34;https://www.google.com/finance/historical?&#34;</span>
</span></span><span class=line><span class=cl>           <span class=s2>&#34;output=csv&amp;q=</span><span class=si>{0}</span><span class=s2>&amp;startdate=Jan+1%2C+1980&amp;enddate=</span><span class=si>{1}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=n>symbol_url</span> <span class=o>=</span> <span class=n>BASE_URL</span><span class=o>.</span><span class=n>format</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>urllib2</span><span class=o>.</span><span class=n>quote</span><span class=p>(</span><span class=s1>&#39;GOOG&#39;</span><span class=p>),</span> <span class=c1># Replace with any stock you are interested.</span>
</span></span><span class=line><span class=cl>    <span class=n>urllib2</span><span class=o>.</span><span class=n>quote</span><span class=p>(</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=o>.</span><span class=n>strftime</span><span class=p>(</span><span class=s2>&#34;%b+</span><span class=si>%d</span><span class=s2>,+%Y&#34;</span><span class=p>),</span> <span class=s1>&#39;+&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>When fetching the content, remember to add try-catch wrapper in case the link fails or the provided stock symbol is not valid.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1>1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2>2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3>3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4>4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5>5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6>6</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>f</span> <span class=o>=</span> <span class=n>urllib2</span><span class=o>.</span><span class=n>urlopen</span><span class=p>(</span><span class=n>symbol_url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;GOOG.csv&#34;</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>fin</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span> <span class=o>&gt;&gt;</span> <span class=n>fin</span><span class=p>,</span> <span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>except</span> <span class=n>urllib2</span><span class=o>.</span><span class=n>HTTPError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span> <span class=s2>&#34;Fetching Failed: </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>symbol_url</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=model-construction>Model Construction<a hidden class=anchor aria-hidden=true href=#model-construction>#</a></h2><p>The model is expected to learn the price sequences of different stocks in time. Due to the different underlying patterns, I would like to tell the model which stock it is dealing with explicitly. <a href=https://en.wikipedia.org/wiki/Embedding>Embedding</a> is more favored than one-hot encoding, because:</p><ol><li>Given that the train set includes $N$ stocks, the one-hot encoding would introduce $N$ (or $N-1$) additional sparse feature dimensions. Once each stock symbol is mapped onto a much smaller embedding vector of length $k$, $k \ll N$, we end up with a much more compressed representation and smaller dataset to take care of.</li><li>Since embedding vectors are variables to learn. Similar stocks could be associated with similar embeddings and help the prediction of each others, such as “GOOG” and “GOOGL” which you will see in Fig. 5. later.</li></ol><p>In the recurrent neural network, at one time step $t$, the input vector contains <code>input_size</code> (labelled as $w$) daily price values of $i$-in stock, ($p_{i, t w}, p_{i, t w+1}, \ldots, p_{i,(t+1) w-1}$). The stock symbol is uniquely mapped to a vector of length embedding<em>size (labelled as $k$), $\left(e</em>{i, 0}, e*{i, 1}, \ldots, e*{i, k}\right)$. As illustrated in Fig. 1., the price vector is concatenated with the embedding vector and then fed into the LSTM cell.</p><p>Another alternative is to concatenate the embedding vectors with the last state of the LSTM cell and learn new weights $W$ and bias $b$ in the output layer. However, in this way, the LSTM cell cannot tell apart prices of one stock from another and its power would be largely restrained. Thus I decided to go with the former approach.</p><p><img loading=lazy src=/post/2017-07-22-stock-rnn-part-2/rnn_with_embedding.png alt="The Architecture of the Stock Price Prediction RNN Model with Stock Symbol Embeddings"></p><p><em>Fig. 1: The architecture of the stock price prediction RNN model with stock symbol embeddings.</em></p><p>Two new configuration settings are added into <code>RNNConfig</code>:</p><ul><li><code>embedding_size</code> controls the size of each embedding vector;</li><li><code>stock_count</code> refers to the number of unique stocks in the dataset.</li></ul><p>Together they define the size of the embedding matrix, for which the model has to learn embedding_size $\times $ stock_count additional variables compared to the model in <a href=https://jarvisma.xyz/posts/predict-stock-prices-using-rnn-part1/>Part 1</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span><span class=lnt id=hl-2-4><a class=lnlinks href=#hl-2-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RNNConfig</span><span class=p>():</span>
</span></span><span class=line><span class=cl>   <span class=c1># ... old ones</span>
</span></span><span class=line><span class=cl>   <span class=n>embedding_size</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl>   <span class=n>stock_count</span> <span class=o>=</span> <span class=mi>50</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=define-the-graph>Define the Graph<a hidden class=anchor aria-hidden=true href=#define-the-graph>#</a></h3><p><strong>— Let’s start going through some code —</strong></p><p>(1) As demonstrated in tutorial <a href=https://jarvisma.xyz/posts/predict-stock-prices-using-rnn-part1/#define-graph>Part 1: Define the Graph</a>, let us define a <code>tf.Graph()</code> named <code>lstm_graph</code> and a set of tensors to hold input data, <code>inputs</code>, <code>targets</code>, and <code>learning_rate</code> in the same way. One more placeholder to define is a list of stock symbols associated with the input prices. Stock symbols have been mapped to unique integers beforehand with <a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html>label encoding</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1>1</a>
</span><span class=lnt id=hl-3-2><a class=lnlinks href=#hl-3-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Mapped to an integer. one label refers to one stock symbol.</span>
</span></span><span class=line><span class=cl><span class=n>stock_labels</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>int32</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=mi>1</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>(2) Then we need to set up an embedding matrix to play as a lookup table, containing the embedding vectors of all the stocks. The matrix is initialized with random numbers in the interval [-1, 1] and gets updated during training.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=n>Then</span> <span class=n>we</span> <span class=n>need</span> <span class=n>to</span> <span class=nb>set</span> <span class=n>up</span> <span class=n>an</span> <span class=n>embedding</span> <span class=n>matrix</span> <span class=n>to</span> <span class=n>play</span> <span class=k>as</span> <span class=n>a</span> <span class=n>lookup</span> <span class=n>table</span><span class=p>,</span> <span class=n>containing</span> <span class=n>the</span> <span class=n>embedding</span> <span class=n>vectors</span> <span class=n>of</span> <span class=nb>all</span> <span class=n>the</span> <span class=n>stocks</span><span class=o>.</span> <span class=n>The</span> <span class=n>matrix</span> <span class=ow>is</span> <span class=n>initialized</span> <span class=k>with</span> <span class=n>random</span> <span class=n>numbers</span> <span class=ow>in</span> <span class=n>the</span> <span class=n>interval</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span> <span class=ow>and</span> <span class=n>gets</span> <span class=n>updated</span> <span class=n>during</span> <span class=n>training</span><span class=o>.</span>
</span></span></code></pre></td></tr></table></div></div><p>(3) Repeat the stock labels <code>num_steps</code> times to match the unfolded version of RNN and the shape of <code>inputs</code> tensor during training. The transformation operation <a href=https://www.tensorflow.org/api_docs/python/tf/tile>tf.tile</a> receives a base tensor and creates a new tensor by replicating its certain dimensions multiples times; precisely the $i$-th dimension of the input tensor gets multiplied by <code>multiples[i]</code> times. For example, if the <code>stock_labels</code> is <code>[[0], [0], [2], [1]]</code> tiling it by [1, 5] produces <code>[[0 0 0 0 0], [0 0 0 0 0], [2 2 2 2 2], [1 1 1 1 1]]</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>stacked_stock_labels</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>tile</span><span class=p>(</span><span class=n>stock_labels</span><span class=p>,</span> <span class=n>multiples</span><span class=o>=</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>num_steps</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>(4) Then we map the symbols to embedding vectors according to the lookup table <code>embedding_matrix</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-6-1><a class=lnlinks href=#hl-6-1>1</a>
</span><span class=lnt id=hl-6-2><a class=lnlinks href=#hl-6-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># stock_label_embeds.get_shape() = (?, num_steps, embedding_size).</span>
</span></span><span class=line><span class=cl><span class=n>stock_label_embeds</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>embedding_lookup</span><span class=p>(</span><span class=n>embedding_matrix</span><span class=p>,</span> <span class=n>stacked_stock_labels</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>(5) Finally, combine the price values with the embedding vectors. The operation <a href=https://www.tensorflow.org/api_docs/python/tf/concat>tf.concat</a> concatenates a list of tensors along the dimension <code>axis</code>. In our case, we want to keep the batch size and the number of steps unchanged, but only extend the input vector of length <code>input_size</code> to include embedding features.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-7-1><a class=lnlinks href=#hl-7-1>1</a>
</span><span class=lnt id=hl-7-2><a class=lnlinks href=#hl-7-2>2</a>
</span><span class=lnt id=hl-7-3><a class=lnlinks href=#hl-7-3>3</a>
</span><span class=lnt id=hl-7-4><a class=lnlinks href=#hl-7-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># inputs.get_shape() = (?, num_steps, input_size)</span>
</span></span><span class=line><span class=cl><span class=c1># stock_label_embeds.get_shape() = (?, num_steps, embedding_size)</span>
</span></span><span class=line><span class=cl><span class=c1># inputs_with_embeds.get_shape() = (?, num_steps, input_size + embedding_size)</span>
</span></span><span class=line><span class=cl><span class=n>inputs_with_embeds</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>concat</span><span class=p>([</span><span class=n>inputs</span><span class=p>,</span> <span class=n>stock_label_embeds</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The rest of code runs the dynamic RNN, extracts the last state of the LSTM cell, and handles weights and bias in the output layer. See <a href=https://jarvisma.xyz/posts/predict-stock-prices-using-rnn-part1/#define-graph>Part 1: Define the Graph</a> for the details.</p><h3 id=training-session>Training Session<a hidden class=anchor aria-hidden=true href=#training-session>#</a></h3><p>Please read <a href=https://jarvisma.xyz/posts/predict-stock-prices-using-rnn-part1/#start-training-session>Part 1: Start Training Session</a> if you haven’t for how to run a training session in Tensorflow.</p><p>Before feeding the data into the graph, the stock symbols should be transformed to unique integers with <a href=http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html>label encoding</a>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-8-1><a class=lnlinks href=#hl-8-1>1</a>
</span><span class=lnt id=hl-8-2><a class=lnlinks href=#hl-8-2>2</a>
</span><span class=lnt id=hl-8-3><a class=lnlinks href=#hl-8-3>3</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>LabelEncoder</span>
</span></span><span class=line><span class=cl><span class=n>label_encoder</span> <span class=o>=</span> <span class=n>LabelEncoder</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>label_encoder</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>list_of_symbols</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>The train/test split ratio remains same, 90% for training and 10% for testing, for every individual stock.</p><h3 id=visualize-the-graph>Visualize the Graph<a hidden class=anchor aria-hidden=true href=#visualize-the-graph>#</a></h3><p>After the graph is defined in code, let us check the visualization in Tensorboard to make sure that components are constructed correctly. Essentially it looks very much like our architecture illustration in Fig. 1.</p><p><img loading=lazy src=/post/2017-07-22-stock-rnn-part-2/rnn_with_embedding_tensorboard.png alt="TensorBoard Visualization of the Graph"></p><p><em>Fig. 2: TensorBoard visualization of the graph defined above. Two modules, &ldquo;train&rdquo; and &ldquo;save,&rdquo; have been removed from the main graph.</em></p><p>Other than presenting the graph structure or tracking the variables in time, Tensorboard also supports <strong><a href=https://www.tensorflow.org/get_started/embedding_viz>embeddings visualization</a></strong>. In order to communicate the embedding values to Tensorboard, we need to add proper tracking in the training logs.</p><p>(0) In my embedding visualization, I want to color each stock with its industry sector. This metadata should stored in a csv file. The file has two columns, the stock symbol and the industry sector. It does not matter whether the csv file has header, but the order of the listed stocks must be consistent with <code>label_encoder.classes_</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-9-1><a class=lnlinks href=#hl-9-1>1</a>
</span><span class=lnt id=hl-9-2><a class=lnlinks href=#hl-9-2>2</a>
</span><span class=lnt id=hl-9-3><a class=lnlinks href=#hl-9-3>3</a>
</span><span class=lnt id=hl-9-4><a class=lnlinks href=#hl-9-4>4</a>
</span><span class=lnt id=hl-9-5><a class=lnlinks href=#hl-9-5>5</a>
</span><span class=lnt id=hl-9-6><a class=lnlinks href=#hl-9-6>6</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>csv</span>
</span></span><span class=line><span class=cl><span class=n>embedding_metadata_path</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>your_log_file_folder</span><span class=p>,</span> <span class=s1>&#39;metadata.csv&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>embedding_metadata_path</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>fout</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>csv_writer</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>writer</span><span class=p>(</span><span class=n>fout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># write the content into the csv file.</span>
</span></span><span class=line><span class=cl>    <span class=c1># for example, csv_writer.writerows([&#34;GOOG&#34;, &#34;information_technology&#34;])</span>
</span></span></code></pre></td></tr></table></div></div><p>(1) Set up the summary writer first within the training <code>tf.Session</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-10-1><a class=lnlinks href=#hl-10-1>1</a>
</span><span class=lnt id=hl-10-2><a class=lnlinks href=#hl-10-2>2</a>
</span><span class=lnt id=hl-10-3><a class=lnlinks href=#hl-10-3>3</a>
</span><span class=lnt id=hl-10-4><a class=lnlinks href=#hl-10-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tensorflow.contrib.tensorboard.plugins</span> <span class=kn>import</span> <span class=n>projector</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span><span class=p>(</span><span class=n>graph</span><span class=o>=</span><span class=n>lstm_graph</span><span class=p>)</span> <span class=k>as</span> <span class=n>sess</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>summary_writer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>FileWriter</span><span class=p>(</span><span class=n>your_log_file_folder</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>summary_writer</span><span class=o>.</span><span class=n>add_graph</span><span class=p>(</span><span class=n>sess</span><span class=o>.</span><span class=n>graph</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>(2) Add the tensor <code>embedding_matrix</code> defined in our graph <code>lstm_graph</code> into the projector config variable and attach the metadata csv file.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-11-1><a class=lnlinks href=#hl-11-1>1</a>
</span><span class=lnt id=hl-11-2><a class=lnlinks href=#hl-11-2>2</a>
</span><span class=lnt id=hl-11-3><a class=lnlinks href=#hl-11-3>3</a>
</span><span class=lnt id=hl-11-4><a class=lnlinks href=#hl-11-4>4</a>
</span><span class=lnt id=hl-11-5><a class=lnlinks href=#hl-11-5>5</a>
</span><span class=lnt id=hl-11-6><a class=lnlinks href=#hl-11-6>6</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>projector_config</span> <span class=o>=</span> <span class=n>projector</span><span class=o>.</span><span class=n>ProjectorConfig</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=c1># You can add multiple embeddings. Here we add only one.</span>
</span></span><span class=line><span class=cl>    <span class=n>added_embedding</span> <span class=o>=</span> <span class=n>projector_config</span><span class=o>.</span><span class=n>embeddings</span><span class=o>.</span><span class=n>add</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>added_embedding</span><span class=o>.</span><span class=n>tensor_name</span> <span class=o>=</span> <span class=n>embedding_matrix</span><span class=o>.</span><span class=n>name</span>
</span></span><span class=line><span class=cl>    <span class=c1># Link this tensor to its metadata file.</span>
</span></span><span class=line><span class=cl>    <span class=n>added_embedding</span><span class=o>.</span><span class=n>metadata_path</span> <span class=o>=</span> <span class=n>embedding_metadata_path</span>
</span></span></code></pre></td></tr></table></div></div><p>(3) This line creates a file <code>projector_config.pbtxt</code> in the folder <code>your_log_file_folder</code>. TensorBoard will read this file during startup.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-12-1><a class=lnlinks href=#hl-12-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>projector</span><span class=o>.</span><span class=n>visualize_embeddings</span><span class=p>(</span><span class=n>summary_writer</span><span class=p>,</span> <span class=n>projector_config</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>The model is trained with top 50 stocks with largest market values in the S&amp;P 500 index.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-13-1><a class=lnlinks href=#hl-13-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>python</span> <span class=n>main</span><span class=o>.</span><span class=n>py</span> <span class=o>--</span><span class=n>stock_count</span><span class=o>=</span><span class=mi>50</span> <span class=o>--</span><span class=n>embed_size</span><span class=o>=</span><span class=mi>3</span> <span class=o>--</span><span class=n>input_size</span><span class=o>=</span><span class=mi>3</span> <span class=o>--</span><span class=n>max_epoch</span><span class=o>=</span><span class=mi>50</span> <span class=o>--</span><span class=n>train</span>
</span></span></code></pre></td></tr></table></div></div><p>And the following configuration is used:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-14-1><a class=lnlinks href=#hl-14-1> 1</a>
</span><span class=lnt id=hl-14-2><a class=lnlinks href=#hl-14-2> 2</a>
</span><span class=lnt id=hl-14-3><a class=lnlinks href=#hl-14-3> 3</a>
</span><span class=lnt id=hl-14-4><a class=lnlinks href=#hl-14-4> 4</a>
</span><span class=lnt id=hl-14-5><a class=lnlinks href=#hl-14-5> 5</a>
</span><span class=lnt id=hl-14-6><a class=lnlinks href=#hl-14-6> 6</a>
</span><span class=lnt id=hl-14-7><a class=lnlinks href=#hl-14-7> 7</a>
</span><span class=lnt id=hl-14-8><a class=lnlinks href=#hl-14-8> 8</a>
</span><span class=lnt id=hl-14-9><a class=lnlinks href=#hl-14-9> 9</a>
</span><span class=lnt id=hl-14-10><a class=lnlinks href=#hl-14-10>10</a>
</span><span class=lnt id=hl-14-11><a class=lnlinks href=#hl-14-11>11</a>
</span><span class=lnt id=hl-14-12><a class=lnlinks href=#hl-14-12>12</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>stock_count</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>input_size</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=n>embed_size</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=n>num_steps</span> <span class=o>=</span> <span class=mi>30</span>
</span></span><span class=line><span class=cl><span class=n>lstm_size</span> <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl><span class=n>num_layers</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>max_epoch</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl><span class=n>keep_prob</span> <span class=o>=</span> <span class=mf>0.8</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=n>init_learning_rate</span> <span class=o>=</span> <span class=mf>0.05</span>
</span></span><span class=line><span class=cl><span class=n>learning_rate_decay</span> <span class=o>=</span> <span class=mf>0.99</span>
</span></span><span class=line><span class=cl><span class=n>init_epoch</span> <span class=o>=</span> <span class=mi>5</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=price-prediction>Price Prediction<a hidden class=anchor aria-hidden=true href=#price-prediction>#</a></h3><p>As a brief overview of the prediction quality, Fig. 3 plots the predictions for test data of “KO”, “AAPL”, “GOOG” and “NFLX”. The overall trends matched up between the true values and the predictions. Considering how the prediction task is designed, the model relies on all the historical data points to predict only next 5 (<code>input_size</code>) days. With a small <code>input_size</code>, the model does not need to worry about the long-term growth curve. Once we increase <code>input_size</code>, the prediction would be much harder.</p><p><img loading=lazy src=/post/2017-07-22-stock-rnn-part-2/rnn_embedding_AAPL.png alt="RNN Embedding for AAPL"></p><p><img loading=lazy src=/post/2017-07-22-stock-rnn-part-2/rnn_embedding_MSFT.png alt="RNN Embedding for MSFT"></p><p><img loading=lazy src=/post/2017-07-22-stock-rnn-part-2/rnn_embedding_GOOG.png alt="True and Predicted Stock Prices of AAPL, MSFT, and GOOG"></p><p><em>Fig. 3: True and predicted stock prices of AAPL, MSFT, and GOOG in the test set. The prices are normalized across consecutive prediction sliding windows (See <a href=https://jarvisma.xyz/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/#normalization>Part 1: Normalization</a>.) The y-axis values get multiplied by 5 for a better comparison between true and predicted trends.</em></p><h3 id=embedding-visualization>Embedding Visualization<a hidden class=anchor aria-hidden=true href=#embedding-visualization>#</a></h3><p>One common technique to visualize the clusters in embedding space is <a href=https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding>t-SNE</a> (<a href=http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf>Maaten and Hinton, 2008</a>), which is well supported in Tensorboard. t-SNE, short for “t-Distributed Stochastic Neighbor Embedding, is a variation of Stochastic Neighbor Embedding (<a href=http://www.cs.toronto.edu/~fritz/absps/sne.pdf>Hinton and Roweis, 2002</a>), but with a modified cost function that is easier to optimize.</p><ol><li>Similar to SNE, t-SNE first converts the high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities.</li><li>t-SNE defines a similar probability distribution over the data points in the low-dimensional space, and it minimizes the <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>Kullback–Leibler</a> divergence between the two distributions with respect to the locations of the points on the map.</li></ol><p>Check <a href=http://distill.pub/2016/misread-tsne/>this post</a> for how to adjust the parameters, Perplexity and learning rate (epsilon), in t-SNE visualization.</p><p><img loading=lazy src=/post/2017-07-22-stock-rnn-part-2/embedding_clusters.png alt="Visualization of the Stock Embeddings Using t-SNE"></p><p><em>Fig. 4: Visualization of the stock embeddings using t-SNE. Each label is colored based on the stock industry sector. We have 5 clusters. Interestingly, GOOG, GOOGL, and FB belong to the same cluster, while AMZN and AAPL stay in another.</em></p><p>In the embedding space, we can measure the similarity between two stocks by examining the similarity between their embedding vectors. For example, GOOG is mostly similar to GOOGL in the learned embeddings (See Fig. 5).</p><p><img loading=lazy src=/post/2017-07-22-stock-rnn-part-2/embedding_clusters_2.png alt="Embedding Visualization Highlighting Similar Neighbors for GOOG"></p><p><em>Fig. 5: &ldquo;GOOG&rdquo; is clicked in the embedding visualization graph, and the top 20 similar neighbors are highlighted with colors from dark to light as the similarity decreases.</em></p><h3 id=known-problems>Known Problems<a hidden class=anchor aria-hidden=true href=#known-problems>#</a></h3><ul><li>The prediction values get diminished and flatten quite a lot as the training goes. That’s why I multiplied the absolute values by a constant to make the trend is more visible in Fig. 3., as I’m more curious about whether the prediction on the up-or-down direction right. However, there must be a reason for the diminishing prediction value problem. Potentially rather than using simple MSE as the loss, we can adopt another form of loss function to penalize more when the direction is predicted wrong.</li><li>The loss function decreases fast at the beginning, but it suffers from occasional value explosion (a sudden peak happens and then goes back immediately). I suspect it is related to the form of loss function too. A updated and smarter loss function might be able to resolve the issue.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/tutorial/>Tutorial</a></li><li><a href=http://localhost:1313/tags/rnn/>Rnn</a></li><li><a href=http://localhost:1313/tags/tensorflow/>Tensorflow</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/post/2023-07-29-transformer/the-transformer-blueprint-a-holistic-guide-to-the-transformer-neural-network-architecture/><span class=title>« Prev</span><br><span>The Transformer Blueprint: A Holistic Guide to the Transformer Neural Network Architecture</span>
</a><a class=next href=http://localhost:1313/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/><span class=title>Next »</span><br><span>Predict Stock Prices Using Rnn Part 1</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 2 on x" href="https://x.com/intent/tweet/?text=Predict%20Stock%20Prices%20Using%20Rnn%20Part%202&amp;url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f&amp;hashtags=tutorial%2crnn%2ctensorflow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 2 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f&amp;title=Predict%20Stock%20Prices%20Using%20Rnn%20Part%202&amp;summary=Predict%20Stock%20Prices%20Using%20Rnn%20Part%202&amp;source=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 2 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f&title=Predict%20Stock%20Prices%20Using%20Rnn%20Part%202"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 2 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 2 on whatsapp" href="https://api.whatsapp.com/send?text=Predict%20Stock%20Prices%20Using%20Rnn%20Part%202%20-%20http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 2 on telegram" href="https://telegram.me/share/url?text=Predict%20Stock%20Prices%20Using%20Rnn%20Part%202&amp;url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 2 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Predict%20Stock%20Prices%20Using%20Rnn%20Part%202&u=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-22-stock-rnn-part-2%2fpredict-stock-prices-using-rnn-part2%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Jarvis' Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>