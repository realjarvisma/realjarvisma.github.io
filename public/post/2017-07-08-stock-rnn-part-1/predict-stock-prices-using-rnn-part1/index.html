<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Predict Stock Prices Using Rnn Part 1 | Jarvis' Log</title>
<meta name=keywords content="tutorial,rnn,tensorflow"><meta name=description content="This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. If you don’t know what is recurrent neural network or LSTM cell, feel free to check my previous post.

One thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn’t try hard on improving the prediction outcomes. You are more than welcome to take my code as a reference point and add more stock prediction related ideas to improve it. Enjoy!"><meta name=author content="Jarvis Ma"><link rel=canonical href=http://localhost:1313/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><meta property="og:title" content="Predict Stock Prices Using Rnn Part 1"><meta property="og:description" content="This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. If you don’t know what is recurrent neural network or LSTM cell, feel free to check my previous post.

One thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn’t try hard on improving the prediction outcomes. You are more than welcome to take my code as a reference point and add more stock prediction related ideas to improve it. Enjoy!"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/"><meta property="article:section" content="post"><meta property="article:published_time" content="2017-07-08T18:18:57+08:00"><meta property="article:modified_time" content="2017-07-08T18:18:57+08:00"><meta property="og:site_name" content="Jarvis' Log"><meta name=twitter:card content="summary"><meta name=twitter:title content="Predict Stock Prices Using Rnn Part 1"><meta name=twitter:description content="This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. If you don’t know what is recurrent neural network or LSTM cell, feel free to check my previous post.

One thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn’t try hard on improving the prediction outcomes. You are more than welcome to take my code as a reference point and add more stock prediction related ideas to improve it. Enjoy!"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/post/"},{"@type":"ListItem","position":2,"name":"Predict Stock Prices Using Rnn Part 1","item":"http://localhost:1313/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Predict Stock Prices Using Rnn Part 1","name":"Predict Stock Prices Using Rnn Part 1","description":"This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. If you don’t know what is recurrent neural network or LSTM cell, feel free to check my previous post.\nOne thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn’t try hard on improving the prediction outcomes. You are more than welcome to take my code as a reference point and add more stock prediction related ideas to improve it. Enjoy!\n","keywords":["tutorial","rnn","tensorflow"],"articleBody":"This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. If you don’t know what is recurrent neural network or LSTM cell, feel free to check my previous post.\nOne thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn’t try hard on improving the prediction outcomes. You are more than welcome to take my code as a reference point and add more stock prediction related ideas to improve it. Enjoy!\nOverview of Existing Tutorials There are many tutorials on the Internet, like:\nA noob’s guide to implementing RNN-LSTM using Tensorflow TensorFlow RNN Tutorial LSTM by Example using Tensorflow How to build a Recurrent Neural Network in TensorFlow RNNs in Tensorflow, a Practical Guide and Undocumented Features Sequence prediction using recurrent neural networks(LSTM) with TensorFlow Anyone Can Learn To Code an LSTM-RNN in Python How to do time series prediction using RNNs, TensorFlow and Cloud ML Engine Despite all these existing tutorials, I still want to write a new one mainly for three reasons:\nEarly tutorials cannot cope with the new version any more, as Tensorflow is still under development and changes on API interfaces are being made fast. Many tutorials use synthetic data in the examples. Well, I would like to play with the real world data. Some tutorials assume that you have known something about Tensorflow API beforehand, which makes the reading a bit difficult. After reading a bunch of examples, I would like to suggest taking the official example on Penn Tree Bank (PTB) dataset as your starting point. The PTB example showcases a RNN model in a pretty and modular design pattern, but it might prevent you from easily understanding the model structure. Hence, here I will build up the graph in a very straightforward manner.\nThe Goal I will explain how to build an RNN model with LSTM cells to predict the prices of S\u0026P500 index. The dataset can be downloaded from Yahoo! Finance ^GSPC. In the following example, I used S\u0026P 500 data from Jan 3, 1950 (the maximum date that Yahoo! Finance is able to trace back to) to Jun 23, 2017. The dataset provides several price points per day. For simplicity, we will only use the daily close prices for prediction. Meanwhile, I will demonstrate how to use TensorBoard for easily debugging and model tracking.\nAs a quick recap: the recurrent neural network (RNN) is a type of artificial neural network with self-loop in its hidden layer(s), which enables RNN to use the previous state of the hidden neuron(s) to learn the current state given the new input. RNN is good at processing sequential data. Long short-term memory (LSTM) cell is a specially designed working unit that helps RNN better memorize the long-term context.\nFor more information in depth, please read my previous post or this awesome post.\nData Preparation The stock prices is a time series of length \\(N\\), defined as \\(p_{0},p_{1},\\ldots,p_{N-1}\\) in which \\(p_i\\) is the close price on day \\(i\\), \\(0 \\leq i \u003c N \\). Imagine that we have a sliding window of a fixed size \\(w\\) (later, we refer to this as input_size) and every time we move the window to the right by size \\(w\\), so that there is no overlap between data in all the sliding windows.\nFig. 1: The S\u0026P 500 prices in time. We use content in one sliding window to make predictions for the next, with no overlap between two consecutive windows.\nThe RNN model we are about to build has LSTM cells as basic hidden units. We use values from the very beginning in the first sliding window \\(W_0\\) to the window \\(W_T\\) at time \\(t\\):\n$$W_{0} = \\left(p_{0}, p_{1}, \\ldots, p_{w-1}\\right)$$ $$W_{1} = \\left(p_{w}, p_{w+1}, \\ldots, p_{2 w-1}\\right)$$ $$\\vdots$$ $$W_{t} = \\left(p_{t w}, p_{t w+1}, \\ldots, p_{(t+1) w-1}\\right)$$\nto predict the prices in the following window \\(w_{t+1}\\):\n\\[W_{t+1}=(p_{(t+1)w}, p_{(t+1)w+1}, \\ldots, p_{(t+2)w-1})\\]\nEssentially we try to learn an approximation function, \\(f(W_{0},W_{1},\\ldots,W_{t})\\approx W_{t+1}\\).\nFig. 2: The unrolled version of RNN.\nConsidering how back propagation through time (BPTT) works, we usually train RNN in a “unrolled” version so that we don’t have to do propagation computation too far back and save the training complication.\nHere is the explanation on num_steps from Tensorflow’s tutorial:\nBy design, the output of a recurrent neural network (RNN) depends on arbitrarily distant inputs. Unfortunately, this makes backpropagation computation difficult. In order to make the learning process tractable, it is common practice to create an “unrolled” version of the network, which contains a fixed number (num_steps) of LSTM inputs and outputs. The model is then trained on this finite approximation of the RNN. This can be implemented by feeding inputs of length num_steps at a time and performing a backward pass after each such input block.\nThe sequence of prices are first split into non-overlapped small windows. Each contains input_size numbers and each is considered as one independent input element. Then any num_steps consecutive input elements are grouped into one training input, forming an “un-rolled” version of RNN for training on Tensorfow. The corresponding label is the input element right after them.\nFor instance, if input_size=3 and num_steps=2, my first few training examples would look like:\n$$\\text{Input}_1=[[p_0, p_1, p_2],[p_3, p_4, p_5]]$$ $$\\text{Label}_1=[p_6, p_7, p_8]$$\n$$\\text{Input}_2=[[p_3, p_4, p_5],[p_6, p_7, p_8]]$$ $$\\text{Label}_2=[p_9, p_10, p_11]$$\n$$\\text{Input}_3=[[p_6, p_7, p_8],[p_9, p_10, p_11]]$$ $$\\text{Label}_1=[p_12, p_13, p_14]$$\nHere is the key part for formatting the data:\n1 2 3 4 5 6 seq = [np.array(seq[i * self.input_size: (i + 1) * self.input_size]) for i in range(len(seq) // self.input_size)] # Split into groups of `num_steps` X = np.array([seq[i: i + self.num_steps] for i in range(len(seq) - self.num_steps)]) y = np.array([seq[i + self.num_steps] for i in range(len(seq) - self.num_steps)]) Train / Test Split Since we always want to predict the future, we take the latest 10% of data as the test data.\nNormalization The S\u0026P 500 index increases in time, bringing about the problem that most values in the test set are out of the scale of the train set and thus the model has to predict some numbers it has never seen before. Sadly and unsurprisingly, it does a tragic job. See Fig. 3.\nFig. 3: A very sad example when the RNN model has to predict numbers out of the scale of the training data.\nTo solve the out-of-scale issue, I normalize the prices in each sliding window. The task becomes predicting the relative change rates instead of the absolute values. In a normalized sliding window \\(W_{t}^{\\prime}\\) at time \\(t\\), all the values are divided by the last unknown price-the last price in \\(W_{t-1}\\):\n$$ W_{t}^{\\prime}=\\left(\\frac{p_{t w}}{p_{t w-1}}, \\frac{p_{t w+1}}{p_{t w-1}}, \\ldots, \\frac{p_{(t+1) w-1}}{p_{t w-1}}\\right) $$\nHere is a data achive stock-data-realjarvisma.tar.gz of S \u0026 P 500 stock prices I crawled up to Jul, 2017. Feel free to play with it :)\nModel Construction Definitions lstm_size: number of units in one LSTM layer. num_layers: number of stacked LSTM layers. keep_prob: percentage of cell units to keep in the dropout operation. init_learning_rate: the learning rate to start with. learning_rate_decay: decay ratio in later training epochs. init_epoch: number of epochs using the constant init_learning_rate. max_epoch: total number of epochs in training input_size: size of the sliding window / one training data point batch_size: number of data points to use in one mini-batch. The LSTM model has num_layers stacked LSTM layer(s) and each layer contains lstm_size number of LSTM cells. Then a dropout mask with keep probability keep_prob is applied to the output of every LSTM cell. The goal of dropout is to remove the potential strong dependency on one dimension so as to prevent overfitting.\nThe training requires max_epoch epochs in total; an epoch is a single full pass of all the training data points. In one epoch, the training data points are split into mini-batches of size batch_size. We send one mini-batch to the model for one BPTT learning. The learning rate is set to init_learning_rate during the first init_epoch epochs and then decay by learning_rate_decay during every succeeding epoch.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Configuration is wrapped in one object for easy tracking and passing. class RNNConfig(): input_size=1 num_steps=30 lstm_size=128 num_layers=1 keep_prob=0.8 batch_size = 64 init_learning_rate = 0.001 learning_rate_decay = 0.99 init_epoch = 5 max_epoch = 50 config = RNNConfig() Define Graph A tf.Graph is not attached to any real data. It defines the flow of how to process the data and how to run the computation. Later, this graph can be fed with data within a tf.session and at this moment the computation happens for real.\n— Let’s start going through some code —\n(1) Initialize a new graph first.\n1 2 3 import tensorflow as tf tf.reset_default_graph() lstm_graph = tf.Graph() (2) How the graph works should be defined within its scope.\n1 with lstm_graph.as_default(): (3) Define the data required for computation. Here we need three input variables, all defined as tf.placeholder because we don’t know what they are at the graph construction stage.\ninputs: the training data X, a tensor of shape (# data examples, num_steps, input_size); the number of data examples is unknown, so it is None. In our case, it would be batch_size in training session. Check the input format example if confused. targets: the training label y, a tensor of shape (# data examples, input_size). learning_rate: a simple float. 1 2 3 4 5 6 7 8 9 # Dimension = ( # number of data examples, # number of input in one computation step, # number of numbers in one input # ) # We don't know the number of examples beforehand, so it is None. inputs = tf.placeholder(tf.float32, [None, config.num_steps, config.input_size]) targets = tf.placeholder(tf.float32, [None, config.input_size]) learning_rate = tf.placeholder(tf.float32, None) (4) This function returns one LSTMCell with or without dropout operation.\n1 2 3 4 def _create_one_cell(): return tf.contrib.rnn.LSTMCell(config.lstm_size, state_is_tuple=True) if config.keep_prob \u003c 1.0: return tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob) (5) Let’s stack the cells into multiple layers if needed. MultiRNNCell helps connect sequentially multiple simple cells to compose one cell.\n1 2 3 4 cell = tf.contrib.rnn.MultiRNNCell( [_create_one_cell() for _ in range(config.num_layers)], state_is_tuple=True ) if config.num_layers \u003e 1 else _create_one_cell() (6) tf.nn.dynamic_rnn constructs a recurrent neural network specified by cell (RNNCell). It returns a pair of (model outpus, state), where the outputs val is of size (batch_size, num_steps, lstm_size) by default. The state refers to the current state of the LSTM cell, not consumed here.\n1 val, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32) (7) tf.transpose converts the outputs from the dimension (batch_size, num_steps, lstm_size) to (num_steps, batch_size, lstm_size). Then the last output is picked.\n1 2 3 4 5 # Before transpose, val.get_shape() = (batch_size, num_steps, lstm_size) # After transpose, val.get_shape() = (num_steps, batch_size, lstm_size) val = tf.transpose(val, [1, 0, 2]) # last.get_shape() = (batch_size, lstm_size) last = tf.gather(val, int(val.get_shape()[0]) - 1, name=\"last_lstm_output\") (8) Define weights and biases between the hidden and output layers.\n1 2 3 weight = tf.Variable(tf.truncated_normal([config.lstm_size, config.input_size])) bias = tf.Variable(tf.constant(0.1, shape=[config.input_size])) prediction = tf.matmul(last, weight) + bias (9) We use mean square error as the loss metric and the RMSPropOptimizer algorithm for gradient descent optimization.\n1 2 3 loss = tf.reduce_mean(tf.square(prediction - targets)) optimizer = tf.train.RMSPropOptimizer(learning_rate) minimize = optimizer.minimize(loss) Start Training Session (1) To start training the graph with real data, we need to start a tf.session first.\n1 with tf.Session(graph=lstm_graph) as sess: (2) Initialize the variables as defined.\n1 tf.global_variables_initializer().run() (0) The learning rates for training epochs should have been precomputed beforehand. The index refers to the epoch index.\n1 2 3 4 learning_rates_to_use = [ config.init_learning_rate * ( config.learning_rate_decay ** max(float(i + 1 - config.init_epoch), 0.0) ) for i in range(config.max_epoch)] (3) Each loop below completes one epoch training.\n1 2 3 4 5 6 7 8 9 10 11 12 for epoch_step in range(config.max_epoch): current_lr = learning_rates_to_use[epoch_step] # if you are curious to know what is StockDataSet and how generate_one_epoch() # is implemented. for batch_X, batch_y in stock_dataset.generate_one_epoch(config.batch_size): train_data_feed = { inputs: batch_X, targets: batch_y, learning_rate: current_lr } train_loss, _ = sess.run([loss, minimize], train_data_feed) (4) Don’t forget to save your trained model at the end.\n1 2 saver = tf.train.Saver() saver.save(sess, \"your_awesome_model_path_and_name\", global_step=max_epoch_step) Use TensorBoard Building the graph without visualization is like drawing in the dark, very obscure and error-prone. Tensorboard provides easy visualization of the graph structure and the learning process. Check out this hand-on tutorial, only 20 min, but it is very practical and showcases several live demos.\nBrief Summary\nUse with tf.name_scope(\"your_awesome_module_name\"): to wrap elements working on the similar goal together. Many tf.* methods accepts name= argument. Assigning a customized name can make your life much easier when reading the graph. Methods like tf.summary.scalar and tf.summary.histogram help track the values of variables in the graph during iterations. In the training session, define a log file using tf.summary.FileWriter. 1 2 3 4 with tf.Session(graph=lstm_graph) as sess: merged_summary = tf.summary.merge_all() writer = tf.summary.FileWriter(\"location_for_keeping_your_log_files\", sess.graph) writer.add_graph(sess.graph) Later, write the training progress and summary results into the file.\n1 2 _summary = sess.run([merged_summary], test_data_feed) writer.add_summary(_summary, global_step=epoch_step) # epoch_step in range(config.max_epoch) Fig. 4a: The RNN graph built by the example code. The “train” module has been “removed from the main graph,” as it is not a real part of the model during prediction time.\nFig. 4b: Click the “output_layer” module to expand it and check the structure in detail.\nResults I used the following configuration in the experiment.\n1 2 3 4 5 6 7 8 num_layers=1 keep_prob=0.8 batch_size = 64 init_learning_rate = 0.001 learning_rate_decay = 0.99 init_epoch = 5 max_epoch = 100 num_steps=30 (Thanks to Yury for cathcing a bug that I had in the price normalization. Instead of using the last price of the previous time window, I ended up with using the last price in the same window. The following plots have been corrected.)\nOverall predicting the stock prices is not an easy task. Especially after normalization, the price trends look very noisy.\nFig. 5a: Prediction results for the last 200 days in test data. Model is trained with input_size=1 and lstm_size=32.\nFig. 5b: Prediction results for the last 200 days in test data. Model is trained with input_size=1 and lstm_size=128.\nFig. 5c: Prediction results for the last 200 days in test data. Model is trained with input_size=5, lstm_size=128, and max_epoch=75 (instead of 50).\n(Updated on Sep 14, 2017) The model code has been updated to be wrapped into a class: LstmRNN. The model training can be triggered by main.py, such as:\n1 python main.py --stock_symbol=SP500 --train --input_size=1 --lstm_size=128 ","wordCount":"2432","inLanguage":"en","datePublished":"2017-07-08T18:18:57+08:00","dateModified":"2017-07-08T18:18:57+08:00","author":{"@type":"Person","name":"Jarvis Ma"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1/"},"publisher":{"@type":"Organization","name":"Jarvis' Log","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Jarvis' Log (Alt + H)"><img src=http://localhost:1313/favicon.ico alt aria-label=logo height=15>Jarvis' Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Predict Stock Prices Using Rnn Part 1
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2017-07-08 18:18:57 +0800 CST'>Jul 8, 2017</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;2432 words&nbsp;·&nbsp;Jarvis Ma&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/post/2017-07-08-stock-rnn-part-1/predict-stock-prices-using-rnn-part1.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#overview-of-existing-tutorials>Overview of Existing Tutorials</a></li><li><a href=#the-goal>The Goal</a></li><li><a href=#data-preparation>Data Preparation</a><ul><li><a href=#train--test-split>Train / Test Split</a></li><li><a href=#normalization>Normalization</a></li></ul></li><li><a href=#model-construction>Model Construction</a><ul><li><a href=#definitions>Definitions</a></li><li><a href=#define-graph>Define Graph</a></li><li><a href=#start-training-session>Start Training Session</a></li><li><a href=#use-tensorboard>Use TensorBoard</a></li></ul></li><li><a href=#results>Results</a></li></ul></nav></div></details></div><div class=post-content><p>This is a tutorial for how to build a recurrent neural network using Tensorflow to predict stock market prices. If you don’t know what is recurrent neural network or LSTM cell, feel free to check <a href=https://jarvisma.xyz/posts/an-overview-of-deep-learning-for-curious-people/#recurrent-neural-network>my previous post</a>.</p><blockquote><p>One thing I would like to emphasize that because my motivation for writing this post is more on demonstrating how to build and train an RNN model in Tensorflow and less on solve the stock prediction problem, I didn’t try hard on improving the prediction outcomes. You are more than welcome to take my code as a reference point and add more stock prediction related ideas to improve it. Enjoy!</p></blockquote><h2 id=overview-of-existing-tutorials>Overview of Existing Tutorials<a hidden class=anchor aria-hidden=true href=#overview-of-existing-tutorials>#</a></h2><p>There are many tutorials on the Internet, like:</p><ul><li><a href=http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/>A noob’s guide to implementing RNN-LSTM using Tensorflow</a></li><li><a href=https://svds.com/tensorflow-rnn-tutorial/>TensorFlow RNN Tutorial</a></li><li><a href=https://medium.com/towards-data-science/lstm-by-example-using-tensorflow-feb0c1968537>LSTM by Example using Tensorflow</a></li><li><a href=https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767>How to build a Recurrent Neural Network in TensorFlow</a></li><li><a href=http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/>RNNs in Tensorflow, a Practical Guide and Undocumented Features</a></li><li><a href=http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html>Sequence prediction using recurrent neural networks(LSTM) with TensorFlow</a></li><li><a href=https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/>Anyone Can Learn To Code an LSTM-RNN in Python</a></li><li><a href=https://medium.com/google-cloud/how-to-do-time-series-prediction-using-rnns-and-tensorflow-and-cloud-ml-engine-2ad2eeb189e8>How to do time series prediction using RNNs, TensorFlow and Cloud ML Engine</a></li></ul><p>Despite all these existing tutorials, I still want to write a new one mainly for three reasons:</p><ol><li>Early tutorials cannot cope with the new version any more, as Tensorflow is still under development and changes on API interfaces are being made fast.</li><li>Many tutorials use synthetic data in the examples. Well, I would like to play with the real world data.</li><li>Some tutorials assume that you have known something about Tensorflow API beforehand, which makes the reading a bit difficult.</li></ol><p>After reading a bunch of examples, I would like to suggest taking the <a href=https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb>official example</a> on Penn Tree Bank (PTB) dataset as your starting point. The PTB example showcases a RNN model in a pretty and modular design pattern, but it might prevent you from easily understanding the model structure. Hence, here I will build up the graph in a very straightforward manner.</p><h2 id=the-goal>The Goal<a hidden class=anchor aria-hidden=true href=#the-goal>#</a></h2><p>I will explain how to build an RNN model with LSTM cells to predict the prices of S&amp;P500 index. The dataset can be downloaded from <a href="https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC">Yahoo! Finance ^GSPC</a>. In the following example, I used S&amp;P 500 data from Jan 3, 1950 (the maximum date that Yahoo! Finance is able to trace back to) to Jun 23, 2017. The dataset provides several price points per day. For simplicity, we will only use the daily close prices for prediction. Meanwhile, I will demonstrate how to use <a href=https://www.tensorflow.org/get_started/summaries_and_tensorboard>TensorBoard</a> for easily debugging and model tracking.</p><p>As a quick recap: the recurrent neural network (RNN) is a type of artificial neural network with self-loop in its hidden layer(s), which enables RNN to use the previous state of the hidden neuron(s) to learn the current state given the new input. RNN is good at processing sequential data. Long short-term memory (LSTM) cell is a specially designed working unit that helps RNN better memorize the long-term context.</p><p>For more information in depth, please read <a href=https://jarvisma.xyz/posts/en/an-overview-of-deep-learning-for-curious-people/#recurrent-neural-network>my previous post</a> or <a href=http://colah.github.io/posts/2015-08-Understanding-LSTMs/>this awesome post</a>.</p><h2 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h2><p>The stock prices is a time series of length \(N\), defined as \(p_{0},p_{1},\ldots,p_{N-1}\) in which \(p_i\) is the close price on day \(i\), \(0 \leq i &lt; N \). Imagine that we have a sliding window of a fixed size \(w\) (later, we refer to this as <code>input_size</code>) and every time we move the window to the right by size \(w\), so that there is no overlap between data in all the sliding windows.</p><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/sliding_window_time_series.svg alt="The S&amp;P 500 Prices in Time"></p><p><em>Fig. 1: The S&amp;P 500 prices in time. We use content in one sliding window to make predictions for the next, with no overlap between two consecutive windows.</em></p><p>The RNN model we are about to build has LSTM cells as basic hidden units. We use values from the very beginning in the first sliding window \(W_0\) to the window \(W_T\) at time \(t\):</p><p>$$W_{0} = \left(p_{0}, p_{1}, \ldots, p_{w-1}\right)$$
$$W_{1} = \left(p_{w}, p_{w+1}, \ldots, p_{2 w-1}\right)$$
$$\vdots$$
$$W_{t} = \left(p_{t w}, p_{t w+1}, \ldots, p_{(t+1) w-1}\right)$$</p><p>to predict the prices in the following window \(w_{t+1}\):</p><p>\[W_{t+1}=(p_{(t+1)w}, p_{(t+1)w+1}, \ldots, p_{(t+2)w-1})\]</p><p>Essentially we try to learn an approximation function, \(f(W_{0},W_{1},\ldots,W_{t})\approx W_{t+1}\).</p><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/unrolled_RNN.png alt="The Unrolled Version of RNN"></p><p><em>Fig. 2: The unrolled version of RNN.</em></p><p>Considering how <a href=https://en.wikipedia.org/wiki/Backpropagation_through_time>back propagation through time (BPTT)</a> works, we usually train RNN in a “unrolled” version so that we don’t have to do propagation computation too far back and save the training complication.</p><p>Here is the explanation on <code>num_steps</code> from <a href=https://tensorflow.org/tutorials/recurrent>Tensorflow’s tutorial</a>:</p><blockquote><p>By design, the output of a recurrent neural network (RNN) depends on arbitrarily distant inputs. Unfortunately, this makes backpropagation computation difficult. In order to make the learning process tractable, it is common practice to create an “unrolled” version of the network, which contains a fixed number (<code>num_steps</code>) of LSTM inputs and outputs. The model is then trained on this finite approximation of the RNN. This can be implemented by feeding inputs of length <code>num_steps</code> at a time and performing a backward pass after each such input block.</p></blockquote><p>The sequence of prices are first split into non-overlapped small windows. Each contains <code>input_size</code> numbers and each is considered as one independent input element. Then any <code>num_steps</code> consecutive input elements are grouped into one training input, forming an <strong>“un-rolled”</strong> version of RNN for training on Tensorfow. The corresponding label is the input element right after them.</p><p>For instance, if <code>input_size=3</code> and <code>num_steps=2</code>, my first few training examples would look like:</p><p>$$\text{Input}_1=[[p_0, p_1, p_2],[p_3, p_4, p_5]]$$
$$\text{Label}_1=[p_6, p_7, p_8]$$</p><p>$$\text{Input}_2=[[p_3, p_4, p_5],[p_6, p_7, p_8]]$$
$$\text{Label}_2=[p_9, p_10, p_11]$$</p><p>$$\text{Input}_3=[[p_6, p_7, p_8],[p_9, p_10, p_11]]$$
$$\text{Label}_1=[p_12, p_13, p_14]$$</p><p>Here is the key part for formatting the data:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-0-1><a class=lnlinks href=#hl-0-1>1</a>
</span><span class=lnt id=hl-0-2><a class=lnlinks href=#hl-0-2>2</a>
</span><span class=lnt id=hl-0-3><a class=lnlinks href=#hl-0-3>3</a>
</span><span class=lnt id=hl-0-4><a class=lnlinks href=#hl-0-4>4</a>
</span><span class=lnt id=hl-0-5><a class=lnlinks href=#hl-0-5>5</a>
</span><span class=lnt id=hl-0-6><a class=lnlinks href=#hl-0-6>6</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>seq</span> <span class=o>=</span> <span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>seq</span><span class=p>[</span><span class=n>i</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>input_size</span><span class=p>:</span> <span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>input_size</span><span class=p>])</span>
</span></span><span class=line><span class=cl>       <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>seq</span><span class=p>)</span> <span class=o>//</span> <span class=bp>self</span><span class=o>.</span><span class=n>input_size</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Split into groups of `num_steps`</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>seq</span><span class=p>[</span><span class=n>i</span><span class=p>:</span> <span class=n>i</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_steps</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>seq</span><span class=p>)</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_steps</span><span class=p>)])</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>seq</span><span class=p>[</span><span class=n>i</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_steps</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>seq</span><span class=p>)</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_steps</span><span class=p>)])</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=train--test-split>Train / Test Split<a hidden class=anchor aria-hidden=true href=#train--test-split>#</a></h3><p>Since we always want to predict the future, we take the <strong>latest 10%</strong> of data as the test data.</p><h3 id=normalization>Normalization<a hidden class=anchor aria-hidden=true href=#normalization>#</a></h3><p>The S&amp;P 500 index increases in time, bringing about the problem that most values in the test set are out of the scale of the train set and thus the model has to predict some numbers it has never seen before. Sadly and unsurprisingly, it does a tragic job. See Fig. 3.</p><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/a_sad_example_stock_prediction.png alt="A Very Sad Example of Stock Prediction"></p><p><em>Fig. 3: A very sad example when the RNN model has to predict numbers out of the scale of the training data.</em></p><p>To solve the out-of-scale issue, I normalize the prices in each sliding window. The task becomes predicting the relative change rates instead of the absolute values. In a normalized sliding window \(W_{t}^{\prime}\) at time \(t\), all the values are divided by the last unknown price-the last price in \(W_{t-1}\):</p><p>$$
W_{t}^{\prime}=\left(\frac{p_{t w}}{p_{t w-1}}, \frac{p_{t w+1}}{p_{t w-1}}, \ldots, \frac{p_{(t+1) w-1}}{p_{t w-1}}\right)
$$</p><p>Here is a data achive <a href="https://drive.google.com/file/d/1yYMDIprJ61Rd6bydrUlEZj_NCL4TtDQ_/view?usp=sharing">stock-data-realjarvisma.tar.gz</a> of S & P 500 stock prices I crawled up to Jul, 2017. Feel free to play with it :)</p><h2 id=model-construction>Model Construction<a hidden class=anchor aria-hidden=true href=#model-construction>#</a></h2><h3 id=definitions>Definitions<a hidden class=anchor aria-hidden=true href=#definitions>#</a></h3><ul><li><code>lstm_size</code>: number of units in one LSTM layer.</li><li><code>num_layers</code>: number of stacked LSTM layers.</li><li><code>keep_prob</code>: percentage of cell units to keep in the <a href=https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf>dropout</a> operation.</li><li><code>init_learning_rate</code>: the learning rate to start with.</li><li><code>learning_rate_decay</code>: decay ratio in later training epochs.</li><li><code>init_epoch</code>: number of epochs using the constant <code>init_learning_rate</code>.</li><li><code>max_epoch</code>: total number of epochs in training</li><li><code>input_size</code>: size of the sliding window / one training data point</li><li><code>batch_size</code>: number of data points to use in one mini-batch.</li></ul><p>The LSTM model has <code>num_layers</code> stacked LSTM layer(s) and each layer contains <code>lstm_size</code> number of LSTM cells. Then a <a href=https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf>dropout</a> mask with keep probability <code>keep_prob</code> is applied to the output of every LSTM cell. The goal of dropout is to remove the potential strong dependency on one dimension so as to prevent overfitting.</p><p>The training requires <code>max_epoch</code> epochs in total; an <a href=http://www.fon.hum.uva.nl/praat/manual/epoch.html>epoch</a> is a single full pass of all the training data points. In one epoch, the training data points are split into mini-batches of size <code>batch_size</code>. We send one mini-batch to the model for one BPTT learning. The learning rate is set to <code>init_learning_rate</code> during the first <code>init_epoch</code> epochs and then decay by <code>learning_rate_decay</code> during every succeeding epoch.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-1-1><a class=lnlinks href=#hl-1-1> 1</a>
</span><span class=lnt id=hl-1-2><a class=lnlinks href=#hl-1-2> 2</a>
</span><span class=lnt id=hl-1-3><a class=lnlinks href=#hl-1-3> 3</a>
</span><span class=lnt id=hl-1-4><a class=lnlinks href=#hl-1-4> 4</a>
</span><span class=lnt id=hl-1-5><a class=lnlinks href=#hl-1-5> 5</a>
</span><span class=lnt id=hl-1-6><a class=lnlinks href=#hl-1-6> 6</a>
</span><span class=lnt id=hl-1-7><a class=lnlinks href=#hl-1-7> 7</a>
</span><span class=lnt id=hl-1-8><a class=lnlinks href=#hl-1-8> 8</a>
</span><span class=lnt id=hl-1-9><a class=lnlinks href=#hl-1-9> 9</a>
</span><span class=lnt id=hl-1-10><a class=lnlinks href=#hl-1-10>10</a>
</span><span class=lnt id=hl-1-11><a class=lnlinks href=#hl-1-11>11</a>
</span><span class=lnt id=hl-1-12><a class=lnlinks href=#hl-1-12>12</a>
</span><span class=lnt id=hl-1-13><a class=lnlinks href=#hl-1-13>13</a>
</span><span class=lnt id=hl-1-14><a class=lnlinks href=#hl-1-14>14</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Configuration is wrapped in one object for easy tracking and passing.</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RNNConfig</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>input_size</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>num_steps</span><span class=o>=</span><span class=mi>30</span>
</span></span><span class=line><span class=cl>    <span class=n>lstm_size</span><span class=o>=</span><span class=mi>128</span>
</span></span><span class=line><span class=cl>    <span class=n>num_layers</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>keep_prob</span><span class=o>=</span><span class=mf>0.8</span>
</span></span><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl>    <span class=n>init_learning_rate</span> <span class=o>=</span> <span class=mf>0.001</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate_decay</span> <span class=o>=</span> <span class=mf>0.99</span>
</span></span><span class=line><span class=cl>    <span class=n>init_epoch</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl>    <span class=n>max_epoch</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>config</span> <span class=o>=</span> <span class=n>RNNConfig</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=define-graph>Define Graph<a hidden class=anchor aria-hidden=true href=#define-graph>#</a></h3><p>A <a href=https://www.tensorflow.org/api_docs/python/tf/Graph>tf.Graph</a> is not attached to any real data. It defines the flow of how to process the data and how to run the computation. Later, this graph can be fed with data within a <a href=https://www.tensorflow.org/api_docs/python/tf/Session>tf.session</a> and at this moment the computation happens for real.</p><p><strong>— Let’s start going through some code —</strong></p><p>(1) Initialize a new graph first.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-2-1><a class=lnlinks href=#hl-2-1>1</a>
</span><span class=lnt id=hl-2-2><a class=lnlinks href=#hl-2-2>2</a>
</span><span class=lnt id=hl-2-3><a class=lnlinks href=#hl-2-3>3</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span><span class=line><span class=cl><span class=n>tf</span><span class=o>.</span><span class=n>reset_default_graph</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>lstm_graph</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Graph</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>(2) How the graph works should be defined within its scope.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-3-1><a class=lnlinks href=#hl-3-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>lstm_graph</span><span class=o>.</span><span class=n>as_default</span><span class=p>():</span>
</span></span></code></pre></td></tr></table></div></div><p>(3) Define the data required for computation. Here we need three input variables, all defined as <a href=https://www.tensorflow.org/versions/master/api_docs/python/tf/placeholder>tf.placeholder</a> because we don’t know what they are at the graph construction stage.</p><ul><li><code>inputs</code>: the training data <em>X</em>, a tensor of shape (# data examples, <code>num_steps</code>, <code>input_size</code>); the number of data examples is unknown, so it is <code>None</code>. In our case, it would be <code>batch_size</code> in training session. Check the <a href=https://jarvisma.xyz/posts/en/predict-stock-prices-using-rnn-part1/#input_format_example>input format example</a> if confused.</li><li><code>targets</code>: the training label y, a tensor of shape (# data examples, <code>input_size</code>).</li><li><code>learning_rate</code>: a simple float.</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-4-1><a class=lnlinks href=#hl-4-1>1</a>
</span><span class=lnt id=hl-4-2><a class=lnlinks href=#hl-4-2>2</a>
</span><span class=lnt id=hl-4-3><a class=lnlinks href=#hl-4-3>3</a>
</span><span class=lnt id=hl-4-4><a class=lnlinks href=#hl-4-4>4</a>
</span><span class=lnt id=hl-4-5><a class=lnlinks href=#hl-4-5>5</a>
</span><span class=lnt id=hl-4-6><a class=lnlinks href=#hl-4-6>6</a>
</span><span class=lnt id=hl-4-7><a class=lnlinks href=#hl-4-7>7</a>
</span><span class=lnt id=hl-4-8><a class=lnlinks href=#hl-4-8>8</a>
</span><span class=lnt id=hl-4-9><a class=lnlinks href=#hl-4-9>9</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=c1># Dimension = (</span>
</span></span><span class=line><span class=cl>    <span class=c1>#     number of data examples,</span>
</span></span><span class=line><span class=cl>    <span class=c1>#     number of input in one computation step,</span>
</span></span><span class=line><span class=cl>    <span class=c1>#     number of numbers in one input</span>
</span></span><span class=line><span class=cl>    <span class=c1># )</span>
</span></span><span class=line><span class=cl>    <span class=c1># We don&#39;t know the number of examples beforehand, so it is None.</span>
</span></span><span class=line><span class=cl>    <span class=n>inputs</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>num_steps</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>input_size</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>targets</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=p>[</span><span class=kc>None</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>input_size</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>placeholder</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>(4) This function returns one <a href=https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/rnn/LSTMCell>LSTMCell</a> with or without dropout operation.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-5-1><a class=lnlinks href=#hl-5-1>1</a>
</span><span class=lnt id=hl-5-2><a class=lnlinks href=#hl-5-2>2</a>
</span><span class=lnt id=hl-5-3><a class=lnlinks href=#hl-5-3>3</a>
</span><span class=lnt id=hl-5-4><a class=lnlinks href=#hl-5-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>_create_one_cell</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>LSTMCell</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>lstm_size</span><span class=p>,</span> <span class=n>state_is_tuple</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>keep_prob</span> <span class=o>&lt;</span> <span class=mf>1.0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>DropoutWrapper</span><span class=p>(</span><span class=n>lstm_cell</span><span class=p>,</span> <span class=n>output_keep_prob</span><span class=o>=</span><span class=n>config</span><span class=o>.</span><span class=n>keep_prob</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>(5) Let’s stack the cells into multiple layers if needed. <code>MultiRNNCell</code> helps connect sequentially multiple simple cells to compose one cell.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-6-1><a class=lnlinks href=#hl-6-1>1</a>
</span><span class=lnt id=hl-6-2><a class=lnlinks href=#hl-6-2>2</a>
</span><span class=lnt id=hl-6-3><a class=lnlinks href=#hl-6-3>3</a>
</span><span class=lnt id=hl-6-4><a class=lnlinks href=#hl-6-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>cell</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>contrib</span><span class=o>.</span><span class=n>rnn</span><span class=o>.</span><span class=n>MultiRNNCell</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=p>[</span><span class=n>_create_one_cell</span><span class=p>()</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>num_layers</span><span class=p>)],</span>
</span></span><span class=line><span class=cl>        <span class=n>state_is_tuple</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=k>if</span> <span class=n>config</span><span class=o>.</span><span class=n>num_layers</span> <span class=o>&gt;</span> <span class=mi>1</span> <span class=k>else</span> <span class=n>_create_one_cell</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>(6) <a href=https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn>tf.nn.dynamic_rnn</a> constructs a recurrent neural network specified by <code>cell</code> (RNNCell). It returns a pair of (model outpus, state), where the outputs <code>val</code> is of size (<code>batch_size</code>, <code>num_steps</code>, <code>lstm_size</code>) by default. The state refers to the current state of the LSTM cell, not consumed here.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-7-1><a class=lnlinks href=#hl-7-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>val</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>dynamic_rnn</span><span class=p>(</span><span class=n>cell</span><span class=p>,</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>(7) <a href=https://www.tensorflow.org/api_docs/python/tf/transpose>tf.transpose</a> converts the outputs from the dimension (<code>batch_size</code>, <code>num_steps</code>, <code>lstm_size</code>) to (<code>num_steps</code>, <code>batch_size</code>, <code>lstm_size</code>). Then the last output is picked.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-8-1><a class=lnlinks href=#hl-8-1>1</a>
</span><span class=lnt id=hl-8-2><a class=lnlinks href=#hl-8-2>2</a>
</span><span class=lnt id=hl-8-3><a class=lnlinks href=#hl-8-3>3</a>
</span><span class=lnt id=hl-8-4><a class=lnlinks href=#hl-8-4>4</a>
</span><span class=lnt id=hl-8-5><a class=lnlinks href=#hl-8-5>5</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=c1># Before transpose, val.get_shape() = (batch_size, num_steps, lstm_size)</span>
</span></span><span class=line><span class=cl>    <span class=c1># After transpose, val.get_shape() = (num_steps, batch_size, lstm_size)</span>
</span></span><span class=line><span class=cl>    <span class=n>val</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=n>val</span><span class=p>,</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=c1># last.get_shape() = (batch_size, lstm_size)</span>
</span></span><span class=line><span class=cl>    <span class=n>last</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>gather</span><span class=p>(</span><span class=n>val</span><span class=p>,</span> <span class=nb>int</span><span class=p>(</span><span class=n>val</span><span class=o>.</span><span class=n>get_shape</span><span class=p>()[</span><span class=mi>0</span><span class=p>])</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=s2>&#34;last_lstm_output&#34;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>(8) Define weights and biases between the hidden and output layers.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-9-1><a class=lnlinks href=#hl-9-1>1</a>
</span><span class=lnt id=hl-9-2><a class=lnlinks href=#hl-9-2>2</a>
</span><span class=lnt id=hl-9-3><a class=lnlinks href=#hl-9-3>3</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>weight</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>truncated_normal</span><span class=p>([</span><span class=n>config</span><span class=o>.</span><span class=n>lstm_size</span><span class=p>,</span> <span class=n>config</span><span class=o>.</span><span class=n>input_size</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=n>bias</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>constant</span><span class=p>(</span><span class=mf>0.1</span><span class=p>,</span> <span class=n>shape</span><span class=o>=</span><span class=p>[</span><span class=n>config</span><span class=o>.</span><span class=n>input_size</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=n>prediction</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>last</span><span class=p>,</span> <span class=n>weight</span><span class=p>)</span> <span class=o>+</span> <span class=n>bias</span>
</span></span></code></pre></td></tr></table></div></div><p>(9) We use mean square error as the loss metric and <a href=http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>the RMSPropOptimizer algorithm</a> for gradient descent optimization.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-10-1><a class=lnlinks href=#hl-10-1>1</a>
</span><span class=lnt id=hl-10-2><a class=lnlinks href=#hl-10-2>2</a>
</span><span class=lnt id=hl-10-3><a class=lnlinks href=#hl-10-3>3</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>loss</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>reduce_mean</span><span class=p>(</span><span class=n>tf</span><span class=o>.</span><span class=n>square</span><span class=p>(</span><span class=n>prediction</span> <span class=o>-</span> <span class=n>targets</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>RMSPropOptimizer</span><span class=p>(</span><span class=n>learning_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>minimize</span> <span class=o>=</span> <span class=n>optimizer</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=start-training-session>Start Training Session<a hidden class=anchor aria-hidden=true href=#start-training-session>#</a></h3><p>(1) To start training the graph with real data, we need to start a <a href=https://www.tensorflow.org/api_docs/python/tf/Session>tf.session</a> first.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-11-1><a class=lnlinks href=#hl-11-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span><span class=p>(</span><span class=n>graph</span><span class=o>=</span><span class=n>lstm_graph</span><span class=p>)</span> <span class=k>as</span> <span class=n>sess</span><span class=p>:</span>
</span></span></code></pre></td></tr></table></div></div><p>(2) Initialize the variables as defined.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-12-1><a class=lnlinks href=#hl-12-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>tf</span><span class=o>.</span><span class=n>global_variables_initializer</span><span class=p>()</span><span class=o>.</span><span class=n>run</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>(0) The learning rates for training epochs should have been precomputed beforehand. The index refers to the epoch index.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-13-1><a class=lnlinks href=#hl-13-1>1</a>
</span><span class=lnt id=hl-13-2><a class=lnlinks href=#hl-13-2>2</a>
</span><span class=lnt id=hl-13-3><a class=lnlinks href=#hl-13-3>3</a>
</span><span class=lnt id=hl-13-4><a class=lnlinks href=#hl-13-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>learning_rates_to_use</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>config</span><span class=o>.</span><span class=n>init_learning_rate</span> <span class=o>*</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>config</span><span class=o>.</span><span class=n>learning_rate_decay</span> <span class=o>**</span> <span class=nb>max</span><span class=p>(</span><span class=nb>float</span><span class=p>(</span><span class=n>i</span> <span class=o>+</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>config</span><span class=o>.</span><span class=n>init_epoch</span><span class=p>),</span> <span class=mf>0.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>max_epoch</span><span class=p>)]</span>
</span></span></code></pre></td></tr></table></div></div><p>(3) Each loop below completes one epoch training.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-14-1><a class=lnlinks href=#hl-14-1> 1</a>
</span><span class=lnt id=hl-14-2><a class=lnlinks href=#hl-14-2> 2</a>
</span><span class=lnt id=hl-14-3><a class=lnlinks href=#hl-14-3> 3</a>
</span><span class=lnt id=hl-14-4><a class=lnlinks href=#hl-14-4> 4</a>
</span><span class=lnt id=hl-14-5><a class=lnlinks href=#hl-14-5> 5</a>
</span><span class=lnt id=hl-14-6><a class=lnlinks href=#hl-14-6> 6</a>
</span><span class=lnt id=hl-14-7><a class=lnlinks href=#hl-14-7> 7</a>
</span><span class=lnt id=hl-14-8><a class=lnlinks href=#hl-14-8> 8</a>
</span><span class=lnt id=hl-14-9><a class=lnlinks href=#hl-14-9> 9</a>
</span><span class=lnt id=hl-14-10><a class=lnlinks href=#hl-14-10>10</a>
</span><span class=lnt id=hl-14-11><a class=lnlinks href=#hl-14-11>11</a>
</span><span class=lnt id=hl-14-12><a class=lnlinks href=#hl-14-12>12</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch_step</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>max_epoch</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>current_lr</span> <span class=o>=</span> <span class=n>learning_rates_to_use</span><span class=p>[</span><span class=n>epoch_step</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># if you are curious to know what is StockDataSet and how generate_one_epoch()</span>
</span></span><span class=line><span class=cl>        <span class=c1># is implemented.</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>batch_X</span><span class=p>,</span> <span class=n>batch_y</span> <span class=ow>in</span> <span class=n>stock_dataset</span><span class=o>.</span><span class=n>generate_one_epoch</span><span class=p>(</span><span class=n>config</span><span class=o>.</span><span class=n>batch_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>train_data_feed</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=n>inputs</span><span class=p>:</span> <span class=n>batch_X</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>targets</span><span class=p>:</span> <span class=n>batch_y</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>learning_rate</span><span class=p>:</span> <span class=n>current_lr</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=n>train_loss</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>([</span><span class=n>loss</span><span class=p>,</span> <span class=n>minimize</span><span class=p>],</span> <span class=n>train_data_feed</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>(4) Don’t forget to save your trained model at the end.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-15-1><a class=lnlinks href=#hl-15-1>1</a>
</span><span class=lnt id=hl-15-2><a class=lnlinks href=#hl-15-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>saver</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>Saver</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>saver</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>sess</span><span class=p>,</span> <span class=s2>&#34;your_awesome_model_path_and_name&#34;</span><span class=p>,</span> <span class=n>global_step</span><span class=o>=</span><span class=n>max_epoch_step</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=use-tensorboard>Use TensorBoard<a hidden class=anchor aria-hidden=true href=#use-tensorboard>#</a></h3><p>Building the graph without visualization is like drawing in the dark, very obscure and error-prone. <a href=https://github.com/tensorflow/tensorboard>Tensorboard</a> provides easy visualization of the graph structure and the learning process. Check out this <a href=https://youtu.be/eBbEDRsCmv4>hand-on tutorial</a>, only 20 min, but it is very practical and showcases several live demos.</p><p><strong>Brief Summary</strong></p><ul><li>Use with <a href=https://www.tensorflow.org/api_docs/python/tf/name_scope>tf.name_scope</a><code>("your_awesome_module_name"):</code> to wrap elements working on the similar goal together.</li><li>Many <code>tf.*</code> methods accepts <code>name=</code> argument. Assigning a customized name can make your life much easier when reading the graph.</li><li>Methods like <a href=https://www.tensorflow.org/api_docs/python/tf/summary/scalar>tf.summary.scalar</a> and <a href=https://www.tensorflow.org/api_docs/python/tf/summary/histogram>tf.summary.histogram</a> help track the values of variables in the graph during iterations.</li><li>In the training session, define a log file using <a href=https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter>tf.summary.FileWriter</a>.</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-16-1><a class=lnlinks href=#hl-16-1>1</a>
</span><span class=lnt id=hl-16-2><a class=lnlinks href=#hl-16-2>2</a>
</span><span class=lnt id=hl-16-3><a class=lnlinks href=#hl-16-3>3</a>
</span><span class=lnt id=hl-16-4><a class=lnlinks href=#hl-16-4>4</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>tf</span><span class=o>.</span><span class=n>Session</span><span class=p>(</span><span class=n>graph</span><span class=o>=</span><span class=n>lstm_graph</span><span class=p>)</span> <span class=k>as</span> <span class=n>sess</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>merged_summary</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>merge_all</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>writer</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>summary</span><span class=o>.</span><span class=n>FileWriter</span><span class=p>(</span><span class=s2>&#34;location_for_keeping_your_log_files&#34;</span><span class=p>,</span> <span class=n>sess</span><span class=o>.</span><span class=n>graph</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>writer</span><span class=o>.</span><span class=n>add_graph</span><span class=p>(</span><span class=n>sess</span><span class=o>.</span><span class=n>graph</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Later, write the training progress and summary results into the file.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-17-1><a class=lnlinks href=#hl-17-1>1</a>
</span><span class=lnt id=hl-17-2><a class=lnlinks href=#hl-17-2>2</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>_summary</span> <span class=o>=</span> <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>([</span><span class=n>merged_summary</span><span class=p>],</span> <span class=n>test_data_feed</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>writer</span><span class=o>.</span><span class=n>add_summary</span><span class=p>(</span><span class=n>_summary</span><span class=p>,</span> <span class=n>global_step</span><span class=o>=</span><span class=n>epoch_step</span><span class=p>)</span>  <span class=c1># epoch_step in range(config.max_epoch)</span>
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/tensorboard1.png alt="The RNN Graph Built by the Example Code"></p><p><em>Fig. 4a: The RNN graph built by the example code. The &ldquo;train&rdquo; module has been &ldquo;removed from the main graph,&rdquo; as it is not a real part of the model during prediction time.</em></p><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/tensorboard2.png alt="Output Layer Structure in TensorBoard"></p><p><em>Fig. 4b: Click the &ldquo;output_layer&rdquo; module to expand it and check the structure in detail.</em></p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>I used the following configuration in the experiment.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-18-1><a class=lnlinks href=#hl-18-1>1</a>
</span><span class=lnt id=hl-18-2><a class=lnlinks href=#hl-18-2>2</a>
</span><span class=lnt id=hl-18-3><a class=lnlinks href=#hl-18-3>3</a>
</span><span class=lnt id=hl-18-4><a class=lnlinks href=#hl-18-4>4</a>
</span><span class=lnt id=hl-18-5><a class=lnlinks href=#hl-18-5>5</a>
</span><span class=lnt id=hl-18-6><a class=lnlinks href=#hl-18-6>6</a>
</span><span class=lnt id=hl-18-7><a class=lnlinks href=#hl-18-7>7</a>
</span><span class=lnt id=hl-18-8><a class=lnlinks href=#hl-18-8>8</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_layers</span><span class=o>=</span><span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>keep_prob</span><span class=o>=</span><span class=mf>0.8</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=n>init_learning_rate</span> <span class=o>=</span> <span class=mf>0.001</span>
</span></span><span class=line><span class=cl><span class=n>learning_rate_decay</span> <span class=o>=</span> <span class=mf>0.99</span>
</span></span><span class=line><span class=cl><span class=n>init_epoch</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=n>max_epoch</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=n>num_steps</span><span class=o>=</span><span class=mi>30</span>
</span></span></code></pre></td></tr></table></div></div><p>(Thanks to Yury for cathcing a bug that I had in the price normalization. Instead of using the last price of the previous time window, I ended up with using the last price in the same window. The following plots have been corrected.)</p><p>Overall predicting the stock prices is not an easy task. Especially after normalization, the price trends look very noisy.</p><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/rnn_input1_lstm32.png alt="Prediction Results for the Last 200 Days in Test Data"></p><p><em>Fig. 5a: Prediction results for the last 200 days in test data. Model is trained with input_size=1 and lstm_size=32.</em></p><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/rnn_input1_lstm128.png alt="Prediction Results for the Last 200 Days in Test Data"></p><p><em>Fig. 5b: Prediction results for the last 200 days in test data. Model is trained with input_size=1 and lstm_size=128.</em></p><p><img loading=lazy src=/post/2017-07-08-stock-rnn-part-1/rnn_input5_lstm128.png alt="Prediction Results for the Last 200 Days in Test Data"></p><p><em>Fig. 5c: Prediction results for the last 200 days in test data. Model is trained with input_size=5, lstm_size=128, and max_epoch=75 (instead of 50).</em></p><p>(Updated on Sep 14, 2017) The model code has been updated to be wrapped into a class: LstmRNN. The model training can be triggered by main.py, such as:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt id=hl-19-1><a class=lnlinks href=#hl-19-1>1</a>
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>python</span> <span class=n>main</span><span class=o>.</span><span class=n>py</span> <span class=o>--</span><span class=n>stock_symbol</span><span class=o>=</span><span class=n>SP500</span> <span class=o>--</span><span class=n>train</span> <span class=o>--</span><span class=n>input_size</span><span class=o>=</span><span class=mi>1</span> <span class=o>--</span><span class=n>lstm_size</span><span class=o>=</span><span class=mi>128</span>
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/tutorial/>Tutorial</a></li><li><a href=http://localhost:1313/tags/rnn/>Rnn</a></li><li><a href=http://localhost:1313/tags/tensorflow/>Tensorflow</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/post/2017-07-22-stock-rnn-part-2/predict-stock-prices-using-rnn-part2/><span class=title>« Prev</span><br><span>Predict Stock Prices Using Rnn Part 2</span>
</a><a class=next href=http://localhost:1313/post/2017-06-21-overview/an-overview-of-deep-learning-for-curious-people/><span class=title>Next »</span><br><span>An Overview of Deep Learning for Curious People</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 1 on x" href="https://x.com/intent/tweet/?text=Predict%20Stock%20Prices%20Using%20Rnn%20Part%201&amp;url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f&amp;hashtags=tutorial%2crnn%2ctensorflow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 1 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f&amp;title=Predict%20Stock%20Prices%20Using%20Rnn%20Part%201&amp;summary=Predict%20Stock%20Prices%20Using%20Rnn%20Part%201&amp;source=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 1 on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f&title=Predict%20Stock%20Prices%20Using%20Rnn%20Part%201"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 1 on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 1 on whatsapp" href="https://api.whatsapp.com/send?text=Predict%20Stock%20Prices%20Using%20Rnn%20Part%201%20-%20http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 1 on telegram" href="https://telegram.me/share/url?text=Predict%20Stock%20Prices%20Using%20Rnn%20Part%201&amp;url=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Predict Stock Prices Using Rnn Part 1 on ycombinator" href="https://news.ycombinator.com/submitlink?t=Predict%20Stock%20Prices%20Using%20Rnn%20Part%201&u=http%3a%2f%2flocalhost%3a1313%2fpost%2f2017-07-08-stock-rnn-part-1%2fpredict-stock-prices-using-rnn-part1%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Jarvis' Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>